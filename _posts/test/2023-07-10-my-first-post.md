---
title:  "[Jekyll] 블로그 포스팅하는 방법"
excerpt: "md 파일에 마크다운 문법으로 작성하여 Github 원격 저장소에 업로드 해보자. 에디터는 Visual Studio code 사용! 로컬 서버에서 확인도 해보자. "

categories:
  - Blog
tags:
  - [Blog, jekyll, Github, Git, AI]

toc: true
toc_sticky: true
 
date: 2023-07-10
last_modified_at: 2023-07-10
---

# ANN

**ANN (Artificial Neural Network)**

- 인공신경망
- 사람의 신경망 원리와 구조를 모방하여 만든 기계학습 알고리즘

인간의 뇌에서 **뉴런들**이 어떤 **신호, 자극** 등을 받고, 그 자극이 어떠한 **임계값(threshold)**을 넘어서면 **결과 신호**를 전달하는 과정에서 착안

- 자극, 신호 → Input Data
- 임계값 → 가중치(weight)
- 자극에 의해 어떤 행동을 하는 것 → Output Data

![ANN%20fc42fbb7194c4400901ea577330d8ede/ANN.jpg](ANN%20fc42fbb7194c4400901ea577330d8ede/ANN.jpg)

- 입력층(Input Layer) : 다수의 입력 데이터
- 출력층(Output Layer) : 데이터의 출력을 담당
- 은닉층(Hidden Layer) : 입력층과 출력층 사이에 존재하는 레이어

은닉층들의 갯수와 노드의 개수를 구성하는 것을 모델을 구성한다고 함 

은닉층에서는 활성화함수를 사용하여 최적의 Weight와 Bias를 찾아내는 역할을 합니다.

이 모델을 잘 구성하여 원하는 Output값을 잘 예측하는 것이 우리가 해야할 일, 

### ANN의 문제점

- **학습과정에서 파라미터의 최적값을 찾기 어렵다.**
    
    출력값을 결정하는 활성화함수의 사용은 기울기 값에 의해 weight가 결정되었는데 이런 gradient값이 뒤로 갈수록 점점 작아져 0에 수렴하는 오류를 낳기도 하고 부분적인 에러를 최저 에러로 인식하여 더이상 학습을 하지 않는 경우도 있습니다.
    
- **Overfitting에 따른 문제**
- **학습시간이 너무 느리다.**
    
    은닉층이 많으면 학습하는데에 정확도가 올라가지만 그만큼 연산량이 기하 급수적으로 늘어나게 됩니다. 하지만 이는 점점 해결되고 있습니다. 느린 학습시간은 그래픽카드의 발전으로 많은 연산량도 감당할 수 있을 정도로 하드웨어의 성능이 좋아졌고, 오버피팅문제는 사전훈련을 통해 방지할 수 있게 되었습니다.
    

# 인공신경망 모형 원리

![ANN%20fc42fbb7194c4400901ea577330d8ede/_2021-05-12__4.54.08.png](ANN%20fc42fbb7194c4400901ea577330d8ede/_2021-05-12__4.54.08.png)

- 활성 함수 란 임계치를 적용해 의미 없는 데이터를 사전에 필터링하고 미분을 편하게 하기 위해서 사용하는 함수다.
- 신경망에서 중요한 것은 가중치를 조절하는 것이다. 어떻게 조절하느냐에 따라 신경망모델의 정확도가 달라지게 된다.
- Feedforward은 입력층, 은닉층, 출력층까지 순서대로 계산하는 방법이고, Backpropagaion은 레이블된 학습 데이터를 가지고 여러 개의 은닉층을 가지는 Feedforward 신경망을 학습시킬 때 사용되는 대표적인 지도 학습 알고리즘이다,
    
    — 즉, forward phase + backword phase 이며, 입력층 → 은닉층 → 출력층 / 출력층 → 은닉층 → 입력층 으로 돌아가며 가중치를 수정하고 위 과정을 반복하며 가장 좋은 결과를 뽑아주는 방법이다.
    
    — forward phase 란 입력층에서 출력층 순서대로 활성화하면서 각 뉴런의 가중치와 활성 함수를 적용하는 것
    
    — backword phase 란 forward phase에서 생성된 결과 신호를 train 데이터의 실제 목표 값과 비교하고, 망의 출력 신호와 실제 값의 차이가 나면 Gradient Descent(경사 하강법)를 이용하여 뉴런간의 가중치를 변경 하는 것이다.
    

※ Gradient Descent(경사 하강법)

최대로 오차를 줄이는 방향으로 가중치를 변경하기 위한 방법이다. 즉, 위의 식에서 W을 수정해나가는 방법론이다. (이로써 신경망의 결과값이 달아진다.

![ANN%20fc42fbb7194c4400901ea577330d8ede/_2021-05-12__5.03.35.png](ANN%20fc42fbb7194c4400901ea577330d8ede/_2021-05-12__5.03.35.png)

 

위의 그림에서의 f(x)는 오차(이전 그림의 f(x), x와는 다름), x는 가중치이다

즉, 가중치를 조절함으로써 오차가 최소가 되도록 값을 조절해야하며 이때 사용하는 것이 경사하강법이다.

이때, 학습률(learing rate)이라는 개념이 사용되는데, 학습률을 너무 높게 주면 건너뛰는 폭이 커진다.

즉, Converagence(최적점)에 도달하기도 전에 높은 학습률 때문에 최적점을 무시하고 다시 높아지는 오차를 가져올 수 있다.

※ Epoch 란?

심층신경망인 DNN 모델링을 할때, Epoch란 인자가 있다. Epoch는 DNN에서 순환하는 과정을 몇번 수행할지 정해주는 인자다. 

심층신경망인 DNN 모델링을 할때, Epoch란 인자가 있다. Epoch는 DNN에서 순환하는 과정을 몇번 수행할지 정해주는 인자다. 

※ 활성화 함수란?

은닉층에 대한 활성화 함수는 네트워크에 비선형성을 적용하는 데 필요하다.

활성화 함수가 적용되고, 결과는 네트워크 내의 다음 뉴런으로 전달된다.

대부분의 비선형 함수들이 사용되는데, 주로 시그모이드 함수가 많이 사용된다.

최근 ReLU함수가 자주 쓰인다.

![ANN%20fc42fbb7194c4400901ea577330d8ede/_2021-05-12__5.24.42.png](ANN%20fc42fbb7194c4400901ea577330d8ede/_2021-05-12__5.24.42.png)

                                <sigmoid 형태의 활성화 함수>

기울기를 이용해 가중치를 업데이트하므로 평평한 활성화 함수는 문제가 있다.

가중치의 변화는 활성화 함수의 기울기에 좌우되기 때문이다. 작은 기울기는 곧 학습 능력이 제한된다는 것을 의미하고 이를 일컬어 신경망에 포화가 발생했다고 한다.

※ 활성화 함수 : ReLU

위에서 보인 sigmoid의 경우 logistic classification에서 어디에 속하는지 분류하기 위해 사용했다

일정값을 넘어야 성공(True)가 될 수 있기 때문에 Activation funtion이라고도 불렀다.

sigmoid는 전달된 값을 0과 1 사일 심하게 변형을 한다. 일정 비율로 줄어들기 때문에 왜국은 아니지만, 값이 현저하게 작아지는 현상이 벌어진다. 3개의 layer를 거치면서 계속해서 1/10로 줄어들었다면, 값이 현저하게 작아지는 현상이 벌어진다. 3개의 layer를 거치면서 계속해서 1/10로 줄어들었다면, 현재 사용 중인 값은 처음 값의 1/1000 이 된다. 이렇게 작아진 값을 갖고 원래의 영향력을 그래도 복원한다는 것은 불가능하다.

이 문제를 vanishing gradient라고 부르고 1986년부터 2006년까지 아무도 해결하지 못했다. Neural Network에 있어 두 번쨰로 찾아온 위기였다. backpropation 에서 전달하는 값은 똑같아 보이지만, layer를 지날 때마다 최초 값보다 현저하게 작아지기 때문에 값을 전달해도 의미를 가질 수가 없었다.

CIFAR에서 지원했던 hinton 교수가 2006년에 방법을 찾아냈다. non-linearity 에 대해 잘못된 방법을 사용했다는 것이었다. non-linearity는 sigmoid 함수를 말한다. vanishing gradient 문제의 발생 원인은 sigmoid 함수에 있었다. sigmoid 함수가 값을 변형하면서 이런 문제가 생긴 것이었다.

![ANN%20fc42fbb7194c4400901ea577330d8ede/_2021-05-13__10.21.55.png](ANN%20fc42fbb7194c4400901ea577330d8ede/_2021-05-13__10.21.55.png)

ReLU 함수는 그림에 있는 것처럼 0보다 작을 때는 0을 사용하고, 0보다 큰 값에 대해서는 해당 값을 그대로 사용하는 방법이다. 음수에 대해서만 값이 바꾸지만, 양수에 대해서는 값을 바꾸지 않는다. 0과 현재 값(x) 중에서 큰 값을 선택하면 끝이다. 코드로 구현하면 max(0, x)이 된다.

ReLU함수는 Rectified Linear Unit의 약자로 retified는 "수정된"이란 뜻을 담고 있다. 즉, 기존의 linear 함수인 sigmoid를 개선했다는 뜻이다.

※ 활성화 함수(Activation Function)의 작동과정

![ANN%20fc42fbb7194c4400901ea577330d8ede/_2021-05-13__10.35.34.png](ANN%20fc42fbb7194c4400901ea577330d8ede/_2021-05-13__10.35.34.png)

위의 그림은 어느 은닉계층의 하나의 뉴런(노드)을 나타낸 것이다. X$_1$, X$_2$, X$_n$은 입력 값들을 나타낸다. 이러한 입력값들에 W$_1$, W$_2$, W$_n$의 가중치들이 곱해지고 이는 f(x)의 함수식을 가진 뉴력에 정달된다. 그리고 이리한 값들이 모두 더해진 값이 분계점(threshold)을 넘어섰을 때, 활성화함수(Activation function)가 작동함으로써 값을 출력하게 된다. 만약 분계점을 충족하지 못하면 해당 뉴런에서 활성화 함수가 작동하지 않고, 출력값 역시 존재하지 않는다.

※ 역전파(Backpropagaion)의 작동과정

![ANN%20fc42fbb7194c4400901ea577330d8ede/_2021-05-13__11.23.37.png](ANN%20fc42fbb7194c4400901ea577330d8ede/_2021-05-13__11.23.37.png)

역전파가 이워지는 과정은 다음과 같다.

1. 최종적으로 출력된 값의 오차를 중이기 위해 바로 이전 단계로 돌아가서 가중치 수정
2. 이때 가중치를 조절하는 방법에는 0.5:0.5로 일정한 비율을 주기도 하고, 실제 가중치에 비례해서 가중치값을 수정한다.
3. 만약 히든레이어가 1개가 아니라 여러 개가 존재한다면, 해단 히든레이어에서 도출된 오차범위가 또 이전단계의 히든레이어로 전달된다. 즉, 출력레이어 이전의 히든레이어에서 멈추는게 아니라, 존대하는 모든 가중치들이 수정된다.

## 신경망 알고리즘의 학습 단계

1. 네트워크 초기화 : 네트워크 가중치의 초깃값이 결정돼야 하는데, 일반적으로는 무작위로 초기화 된다.
2. 피드포워드 : 네트워크를 통해 입력 계층부터, 은닉계층, 출력 계층까지 활성화 함수와 가중치를 적용을 거쳐 정보가 전달된다. 활성화 함수로는 노드 입력의 가중치 합에 대한 시그모이드 함수가 일반적으로 사용된다.
3. 오차평가 : 네트워크에서 계산된 예측 결과를 실제 결과와 비교한다. 둘 사이의 오차가 미리 정해진 가중치보다 작으면 알고리즘은 종료한다.
4. 역전파 : 출력 계층의 오차는 가중치르 다시 조정하는데 사용된다. 알고리즘은 오차를 네트워크에서 거꾸로 전파시키고, 가중치 변경과 관련해서 오차 값 변화의 경사를 계산한다.
5. 조정 : 오차를 줄이는 방향으로 경사의 변화를 사용해서 가중치를 조정한다. 각 뉴런의 가중치와 편향은 활성화함수의 도함수, 신경망의 결과와 실제 결과갑의 차이, 그리고 뉴런 결과 등의 요인에 의해 조정된다. 신경망은 위의 과정을 통해 데이터를 학습하게 된다.

[손으로 직접계산하면서 이해](https://www.notion.so/190898a5a964474e9b02af970b286712?pvs=21)